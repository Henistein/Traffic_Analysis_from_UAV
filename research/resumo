Speed Estimation

Como foi dito antes, o tracking dos veiculus permite-nos saber a posição destes pela sequência de frames do vídeo. Desta forma é possível estimar a velocidade dos veículos. O cálculo da velocidade é dado por:
                                              FORMULA da velocidade
onde Y corresponde ao deslocamento e X ao intervalo de tempo. O tempo pode ser determinado de forma direta, uma vez que sabemos quantas frames por segundo tem o vídeo. A câmera do DJI Phantom 4 v2 grava vídeos a 30 FPS por padrão, mas este valor também consegue ser obtido através de um método da biblioteca OpenCV do Python, getFPS().
Assim para calcular o intervalo de tempo entre frames basta determinar:
                                        segundos por frame = 1/FPS
Para calcular o deslocamento do veículo temos que guardar as sucessivas posições pelas frames do vídeo. De modo a facilitar este processo, nós resumimos o objeto a apenas um ponto. Este ponto pode ser determinado através do cálculo do centroide das bounding boxes, as follows:
                                        FORMULA da centroide
Desta forma conseguimos guardar para cada ID dos veículos a respetiva localização em píxeis entre frames.
                                        IMAGEM?
Uma vez que queremos determinar a velocidade em unidades do SI (m/s? km/h?) temos que converter as coordenadas de píxeis para coordenadas reais (lat e long). De facto, é impossível fazer esta conversão apenas com os dados do vídeo, uma vez que este não guarda a posição da câmera. Porém é possível obter as coordenadas do drone através dos logs de voo, bem como o heading, o pitch e a height. O método proposto para resolver este problema cabe em fazer a georreferenciação das frames do vídeo. A georreferencição de uma imagem é uma técnica que permite relacionar as coordenadas em píxeis para coordenadas reais (lat e long). Esta técnica tem de ser feita manualmente com ajuda de ferramentas tais como o Google Earth e o Qgis e assemalha-se à técnica de Image Registration muito usada na área de computer vision quando se pretende transformar imagens diferentes em apenas um sistema de coordenadas. Por esta razão torna-se quase impossível realizar esta técnica, uma vez que fazer a georreferenciação manual de cada frame de um vídeo implicaria um esforço desnecessário, embora esta fosse a solução ideal. A solução proposta para automatizar este processo foi utilizar apenas uma imagem georreferenciada, sendo esta o Mapa do local onde drone gravou o vídeo. Através dos logs referidos conseguimos determinar em cada frame as coordenadas do drone neste mapa, bem como o seu heading. Com estes dados é possível reproduzir com algum rigor o footprint do drone no mapa, ou seja a área que a câmera do drone consegue capturar. O footprint é dado por:
                                                  IMAGEM EXEMPLO FOOTPRINT

O sensor width e o focal lenght são obtidos a priori através da informação da câmera. Estas informação pode ser encontrada nesta fonte LINK. Ou seja, para o drone utilizado neste problema, Dji Phantom 4 pro v2 os dados da câmera são:
                      TABELA
Para calcular a width e a height do footprint é necessário calcular a Ground Sample Distance (GSD):
                      FORMULA GSD
Para isto funcionar de forma rigoroza os vídeos do drone tiveram de ser gravados com a câmera virada para baixo, ou seja, o pitch = -90º. 
Uma vez que sabemos o heading do drone conseguimos rodar o rectangulo do footprint, de forma a corresponder corretamente com a frame do vídeo.
Um exemplo da representação do footprint do drone no mapa, bem como a frame correspondente é o seguinte:
                      EXEMPLO DO FOOTPRINT
                      FRAME DO VIDEO

Desta forma é possível mapear as coordenadas dos objetos para o mapa georreferenciado. Para tal usámos o recorte do footprint do mapa como intermédio para converter os coordenadas de píxeis para lat e long. O seguinte esquema ilustra a abordagem proposta:
                      ILUSTRACAO DA ABORDAGEM PROPOSTA
                      (por legenda em cada passo)
O algoritmo proposto para converter o sistema de coordenadas consiste em quatro passos: (referenciar cada passo)
  - Scale the frame points to cropped footprint - Os valores do scale que converte um ponto da frame para o footprint são determinados da seguinte forma:
    scaleX = width da frame em pixels / width do footprint em pixels
    scaleY = height da frame em pixels / height do footprint em pixels
    Por fim, a conversão é dada por:
    pt = (pt[0]/ftp_scale_x, pt[1]/ftp_scale_y)
  - Scale the cropped footprint points to map coordintes - De modo a realizar esta conversão é necessário acrescentar à coordenada X dos pontos um valor e à coordenada Y dos pontos outro valor.
     Estes são dados por:
      incx = pixel da coordenada x do drone no mapa - width do footprint em pixels
      incy = pixel da coordenada y do drone no mapa - height do footprint em pixels
     Portando para cada ponto fazemos:
      pt = (pt[0]+incx, pt[1]+incy)

  - Rotate the points - De modo a os pontos combinarem com o heading do drone temos que os rodar. Para isso aplicamos a matriz de rotação a cada ponto de acordo com o valor do angulo heading do drone:
    pt * Matriz rotacao
  - Converter os pixels em coordenadas - Por fim usufruimos do facto da imagem do mapa estar georreferenciada para fazer a conversão dos pixels em coordenadas latitude e longitude.

Com isto é possível estimar a distância em metros que um dado veículo realizou entre frames. Para isto usámos a the Haversine formula as follows:
                  HARVESINVE
Explicar a formula.
Por fim calculamos a velocidade aplicando a fórmula descrita em cima.
  
  
